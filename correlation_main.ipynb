{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaae1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatialTemporalCrossCorrelation:\n",
    "    def __init__(self, video_path, output_dir=None, spatial_grid=(4, 4), temporal_segment=1.0):\n",
    "        \"\"\"\n",
    "        Combined Spatial-Temporal Cross-Correlation Analysis\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video\n",
    "            output_dir: Output directory for results\n",
    "            spatial_grid: Grid division (rows, cols) for spatial analysis\n",
    "            temporal_segment: Duration of each temporal segment in seconds\n",
    "        \"\"\"\n",
    "        self.video_path = video_path\n",
    "        self.video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        self.output_dir = output_dir or os.path.dirname(video_path)\n",
    "        self.spatial_grid = spatial_grid\n",
    "        self.temporal_segment = temporal_segment\n",
    "        self.num_regions = spatial_grid[0] * spatial_grid[1]\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Analysis storage\n",
    "        self.video_info = {}\n",
    "        self.movement_matrix = None  # Regions × Time segments\n",
    "        self.bitrate_matrix = None   # Regions × Time segments\n",
    "        self.spatial_temporal_cross_corr = None\n",
    "        self.combined_correlation_matrices = {}\n",
    "        self.temporal_delay_analysis = {}  # New: Store temporal delay analysis\n",
    "        \n",
    "    def extract_video_information(self):\n",
    "        \"\"\"Extract basic video metadata\"\"\"\n",
    "        print(\"Extracting video information...\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video: {self.video_path}\")\n",
    "        \n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = total_frames / fps if fps > 0 else 0\n",
    "        \n",
    "        self.video_info = {\n",
    "            'width': width,\n",
    "            'height': height, \n",
    "            'fps': fps,\n",
    "            'total_frames': total_frames,\n",
    "            'duration': duration,\n",
    "            'region_width': width // self.spatial_grid[1],\n",
    "            'region_height': height // self.spatial_grid[0],\n",
    "            'frames_per_segment': int(fps * self.temporal_segment),\n",
    "            'total_segments': int(duration / self.temporal_segment)\n",
    "        }\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"Video: {width}x{height}, {fps:.1f} fps, {duration:.1f}s, {total_frames} frames\")\n",
    "        print(f\"Grid: {self.spatial_grid[0]}x{self.spatial_grid[1]} = {self.num_regions} regions\")\n",
    "        print(f\"Temporal: {self.temporal_segment}s segments → {self.video_info['total_segments']} segments\")\n",
    "        \n",
    "        return self.video_info\n",
    "    \n",
    "    def compute_movement_intensity(self):\n",
    "        \"\"\"\n",
    "        Compute movement intensity for each spatial region over time segments\n",
    "        Returns: movement_matrix (regions × time_segments)\n",
    "        \"\"\"\n",
    "        print(\"Computing spatial-temporal movement intensity...\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        fps = self.video_info['fps']\n",
    "        frames_per_segment = self.video_info['frames_per_segment']\n",
    "        total_segments = self.video_info['total_segments']\n",
    "        \n",
    "        # Initialize movement matrix\n",
    "        self.movement_matrix = np.zeros((self.num_regions, total_segments))\n",
    "        \n",
    "        # Motion detection setup\n",
    "        backSub = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=25)\n",
    "        prev_frame = None\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            segment_movement = np.zeros(self.num_regions)\n",
    "            frames_processed = 0\n",
    "            \n",
    "            for frame_idx in range(frames_per_segment):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Convert to grayscale\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Background subtraction\n",
    "                fg_mask = backSub.apply(frame)\n",
    "                fg_mask = cv2.medianBlur(fg_mask, 5)\n",
    "                \n",
    "                # Optical flow for precise motion\n",
    "                if prev_frame is not None:\n",
    "                    flow = cv2.calcOpticalFlowFarneback(\n",
    "                        prev_frame, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "                    )\n",
    "                    flow_magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n",
    "                else:\n",
    "                    flow_magnitude = np.zeros_like(gray, dtype=np.float32)\n",
    "                \n",
    "                # Calculate movement for each region\n",
    "                for region_idx in range(self.num_regions):\n",
    "                    row = region_idx // self.spatial_grid[1]\n",
    "                    col = region_idx % self.spatial_grid[1]\n",
    "                    \n",
    "                    y_start = row * self.video_info['region_height']\n",
    "                    y_end = min((row + 1) * self.video_info['region_height'], self.video_info['height'])\n",
    "                    x_start = col * self.video_info['region_width'] \n",
    "                    x_end = min((col + 1) * self.video_info['region_width'], self.video_info['width'])\n",
    "                    \n",
    "                    # Extract region from mask and flow\n",
    "                    region_mask = fg_mask[y_start:y_end, x_start:x_end]\n",
    "                    region_flow = flow_magnitude[y_start:y_end, x_start:x_end]\n",
    "                    \n",
    "                    # Combined movement intensity\n",
    "                    mask_intensity = np.sum(region_mask > 128)\n",
    "                    flow_intensity = np.sum(region_flow)\n",
    "                    region_area = (y_end - y_start) * (x_end - x_start)\n",
    "                    \n",
    "                    if region_area > 0:\n",
    "                        movement = (0.7 * mask_intensity + 0.3 * flow_intensity) / region_area\n",
    "                        segment_movement[region_idx] += movement\n",
    "                \n",
    "                prev_frame = gray.copy()\n",
    "                frames_processed += 1\n",
    "            \n",
    "            if frames_processed > 0:\n",
    "                self.movement_matrix[:, segment_idx] = segment_movement / frames_processed\n",
    "            \n",
    "            if segment_idx % 10 == 0:\n",
    "                progress = (segment_idx / total_segments) * 100\n",
    "                print(f\"Progress: {progress:.1f}% - Segment {segment_idx}/{total_segments}\")\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Smooth temporal patterns\n",
    "        self.movement_matrix = gaussian_filter1d(self.movement_matrix, sigma=1, axis=1)\n",
    "        \n",
    "        # Normalize each region\n",
    "        for i in range(self.num_regions):\n",
    "            if np.max(self.movement_matrix[i]) > np.min(self.movement_matrix[i]):\n",
    "                self.movement_matrix[i] = (self.movement_matrix[i] - np.min(self.movement_matrix[i])) / \\\n",
    "                                        (np.max(self.movement_matrix[i]) - np.min(self.movement_matrix[i]))\n",
    "        \n",
    "        print(f\"Movement matrix shape: {self.movement_matrix.shape}\")\n",
    "        return self.movement_matrix\n",
    "\n",
    "    def compute_temporal_delay_correlation(self, max_time_lag=10):\n",
    "        \"\"\"\n",
    "        Compute correlation between regions with temporal delays\n",
    "        Analyzes correlation at different time lags from -max_time_lag to +max_time_lag seconds\n",
    "        \n",
    "        Args:\n",
    "            max_time_lag: Maximum time lag in seconds to analyze (positive and negative)\n",
    "        \"\"\"\n",
    "        print(f\"Computing temporal delay correlation analysis (time lags: -{max_time_lag} to +{max_time_lag} seconds)...\")\n",
    "        \n",
    "        if self.movement_matrix is None:\n",
    "            self.compute_movement_intensity()\n",
    "        \n",
    "        num_regions = self.movement_matrix.shape[0]\n",
    "        num_segments = self.movement_matrix.shape[1]\n",
    "        \n",
    "        # Convert time lag to segment lag\n",
    "        max_lag_segments = int(max_time_lag / self.temporal_segment)\n",
    "        total_lags = 2 * max_lag_segments + 1\n",
    "        \n",
    "        print(f\"Time lag: ±{max_time_lag}s → Segment lag: ±{max_lag_segments} segments\")\n",
    "        \n",
    "        # Initialize storage for temporal delay analysis\n",
    "        self.temporal_delay_analysis = {\n",
    "            'max_correlation_matrix': np.zeros((num_regions, num_regions)),\n",
    "            'optimal_time_lag_matrix': np.zeros((num_regions, num_regions)),  # in seconds\n",
    "            'optimal_segment_lag_matrix': np.zeros((num_regions, num_regions)),  # in segments\n",
    "            'lag_correlations': np.zeros((num_regions, num_regions, total_lags)),\n",
    "            'significant_correlations': np.zeros((num_regions, num_regions), dtype=bool),\n",
    "            'high_significant_correlations': np.zeros((num_regions, num_regions), dtype=bool),  # New: for corr > 0.5\n",
    "            'time_lags_seconds': np.linspace(-max_time_lag, max_time_lag, total_lags),\n",
    "            'segment_lags': np.arange(-max_lag_segments, max_lag_segments + 1),\n",
    "            'max_time_lag': max_time_lag,\n",
    "            'max_lag_segments': max_lag_segments\n",
    "        }\n",
    "        \n",
    "        # Define significance thresholds\n",
    "        significance_threshold = 0.3\n",
    "        high_significance_threshold = 0.5  # New: for high correlation\n",
    "        \n",
    "        for i in range(num_regions):\n",
    "            for j in range(num_regions):\n",
    "                if i == j:\n",
    "                    continue  # Skip auto-correlation\n",
    "                \n",
    "                signal_i = self.movement_matrix[i]\n",
    "                signal_j = self.movement_matrix[j]\n",
    "                \n",
    "                # Remove DC component and normalize\n",
    "                signal_i_norm = (signal_i - np.mean(signal_i)) / (np.std(signal_i) + 1e-8)\n",
    "                signal_j_norm = (signal_j - np.mean(signal_j)) / (np.std(signal_j) + 1e-8)\n",
    "                \n",
    "                # Compute cross-correlation for different time lags\n",
    "                lag_correlations = []\n",
    "                \n",
    "                for lag_idx, segment_lag in enumerate(self.temporal_delay_analysis['segment_lags']):\n",
    "                    if segment_lag < 0:\n",
    "                        # Region i leads region j (negative lag: i before j)\n",
    "                        corr_signal_i = signal_i_norm[:segment_lag] if segment_lag != 0 else signal_i_norm\n",
    "                        corr_signal_j = signal_j_norm[-segment_lag:]\n",
    "                    elif segment_lag > 0:\n",
    "                        # Region j leads region i (positive lag: j before i)\n",
    "                        corr_signal_i = signal_i_norm[segment_lag:]\n",
    "                        corr_signal_j = signal_j_norm[:-segment_lag] if segment_lag != 0 else signal_j_norm\n",
    "                    else:\n",
    "                        # No lag\n",
    "                        corr_signal_i = signal_i_norm\n",
    "                        corr_signal_j = signal_j_norm\n",
    "                    \n",
    "                    # Ensure same length\n",
    "                    min_len = min(len(corr_signal_i), len(corr_signal_j))\n",
    "                    if min_len > 10:  # Need sufficient data points\n",
    "                        corr_coef = np.corrcoef(corr_signal_i[:min_len], corr_signal_j[:min_len])[0, 1]\n",
    "                        # Handle NaN values\n",
    "                        if np.isnan(corr_coef):\n",
    "                            corr_coef = 0\n",
    "                        lag_correlations.append(corr_coef)\n",
    "                    else:\n",
    "                        lag_correlations.append(0)\n",
    "                \n",
    "                # Store lag correlations\n",
    "                self.temporal_delay_analysis['lag_correlations'][i, j] = lag_correlations\n",
    "                \n",
    "                # Find maximum correlation and optimal lag\n",
    "                max_corr_idx = np.argmax(np.abs(lag_correlations))\n",
    "                max_correlation = lag_correlations[max_corr_idx]\n",
    "                optimal_segment_lag = self.temporal_delay_analysis['segment_lags'][max_corr_idx]\n",
    "                optimal_time_lag = self.temporal_delay_analysis['time_lags_seconds'][max_corr_idx]\n",
    "                \n",
    "                self.temporal_delay_analysis['max_correlation_matrix'][i, j] = max_correlation\n",
    "                self.temporal_delay_analysis['optimal_segment_lag_matrix'][i, j] = optimal_segment_lag\n",
    "                self.temporal_delay_analysis['optimal_time_lag_matrix'][i, j] = optimal_time_lag\n",
    "                \n",
    "                # Check if correlation is significant\n",
    "                if abs(max_correlation) >= significance_threshold:\n",
    "                    self.temporal_delay_analysis['significant_correlations'][i, j] = True\n",
    "                \n",
    "                # Check if correlation is highly significant (new)\n",
    "                if abs(max_correlation) >= high_significance_threshold:\n",
    "                    self.temporal_delay_analysis['high_significant_correlations'][i, j] = True\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Progress: {i+1}/{num_regions} regions completed\")\n",
    "        \n",
    "        print(\"Temporal delay correlation analysis completed\")\n",
    "        return self.temporal_delay_analysis\n",
    "\n",
    "    def analyze_exact_time_windows(self, correlation_threshold=0.5, window_size_seconds=10):\n",
    "        \"\"\"\n",
    "        Analyze EXACT time windows (0-10s, 10-20s, etc.) where high correlations exist\n",
    "        between specific region pairs\n",
    "        \n",
    "        Args:\n",
    "            correlation_threshold: Minimum correlation to consider as high\n",
    "            window_size_seconds: Size of time windows in seconds\n",
    "        \"\"\"\n",
    "        print(f\"\\nAnalyzing EXACT time windows ({window_size_seconds}-second intervals) with high correlations (> {correlation_threshold})...\")\n",
    "        \n",
    "        if self.movement_matrix is None:\n",
    "            self.compute_movement_intensity()\n",
    "        \n",
    "        if not self.temporal_delay_analysis:\n",
    "            self.compute_temporal_delay_correlation(max_time_lag=10)\n",
    "        \n",
    "        num_segments = self.movement_matrix.shape[1]\n",
    "        segments_per_window = int(window_size_seconds / self.temporal_segment)\n",
    "        num_windows = num_segments // segments_per_window\n",
    "        \n",
    "        print(f\"Time windows: {num_windows} windows of {window_size_seconds} seconds each\")\n",
    "        print(f\"Total analysis duration: {num_windows * window_size_seconds} seconds\")\n",
    "        \n",
    "        # Store time window analysis results\n",
    "        time_window_analysis = {\n",
    "            'window_size_seconds': window_size_seconds,\n",
    "            'correlation_threshold': correlation_threshold,\n",
    "            'windows': [],\n",
    "            'high_correlation_events': []\n",
    "        }\n",
    "        \n",
    "        # Analyze each time window\n",
    "        for window_idx in range(num_windows):\n",
    "            start_segment = window_idx * segments_per_window\n",
    "            end_segment = min((window_idx + 1) * segments_per_window, num_segments)\n",
    "            start_time = window_idx * window_size_seconds\n",
    "            end_time = (window_idx + 1) * window_size_seconds\n",
    "            \n",
    "            window_data = {\n",
    "                'window_id': window_idx,\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'start_segment': start_segment,\n",
    "                'end_segment': end_segment,\n",
    "                'high_correlation_pairs': []\n",
    "            }\n",
    "            \n",
    "            # Find high correlation pairs in this window\n",
    "            for i in range(self.num_regions):\n",
    "                for j in range(self.num_regions):\n",
    "                    if i >= j:  # Avoid duplicates and self-correlation\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract signals for this window\n",
    "                    signal_i_window = self.movement_matrix[i, start_segment:end_segment]\n",
    "                    signal_j_window = self.movement_matrix[j, start_segment:end_segment]\n",
    "                    \n",
    "                    # Compute correlation for this specific window\n",
    "                    if len(signal_i_window) > 5 and len(signal_j_window) > 5:  # Need sufficient data\n",
    "                        window_correlation = np.corrcoef(signal_i_window, signal_j_window)[0, 1]\n",
    "                        \n",
    "                        if not np.isnan(window_correlation) and abs(window_correlation) >= correlation_threshold:\n",
    "                            # Found high correlation in this window\n",
    "                            pair_data = {\n",
    "                                'region_i': i,\n",
    "                                'region_j': j,\n",
    "                                'correlation': float(window_correlation),\n",
    "                                'direction': 'positive' if window_correlation >= 0 else 'negative'\n",
    "                            }\n",
    "                            window_data['high_correlation_pairs'].append(pair_data)\n",
    "                            \n",
    "                            # Also store as event\n",
    "                            time_window_analysis['high_correlation_events'].append({\n",
    "                                'time_window': f\"{start_time}-{end_time}s\",\n",
    "                                'start_time_seconds': start_time,\n",
    "                                'end_time_seconds': end_time,\n",
    "                                'region_i': i,\n",
    "                                'region_j': j,\n",
    "                                'correlation': float(window_correlation),\n",
    "                                'absolute_correlation': float(abs(window_correlation)),\n",
    "                                'direction': 'positive' if window_correlation >= 0 else 'negative'\n",
    "                            })\n",
    "            \n",
    "            time_window_analysis['windows'].append(window_data)\n",
    "        \n",
    "        # Sort high correlation events by correlation strength\n",
    "        time_window_analysis['high_correlation_events'].sort(\n",
    "            key=lambda x: x['absolute_correlation'], reverse=True\n",
    "        )\n",
    "        \n",
    "        self.time_window_analysis = time_window_analysis\n",
    "        print(f\"Found {len(time_window_analysis['high_correlation_events'])} high correlation events across {num_windows} time windows\")\n",
    "        \n",
    "        return time_window_analysis\n",
    "\n",
    "    def create_exact_time_heatmaps(self, correlation_threshold=0.5, window_size_seconds=10):\n",
    "        \"\"\"\n",
    "        Create heatmaps showing EXACT time durations when high correlations occur\n",
    "        between specific region pairs\n",
    "        \"\"\"\n",
    "        print(f\"Creating EXACT time duration heatmaps ({window_size_seconds}-second windows)...\")\n",
    "        \n",
    "        # Perform time window analysis\n",
    "        time_analysis = self.analyze_exact_time_windows(correlation_threshold, window_size_seconds)\n",
    "        \n",
    "        # Create output directory for exact time heatmaps\n",
    "        exact_time_dir = os.path.join(self.output_dir, \"exact_time_correlation_heatmaps\")\n",
    "        os.makedirs(exact_time_dir, exist_ok=True)\n",
    "        \n",
    "        # HEATMAP 1: Time Windows vs Region Pairs\n",
    "        self._create_time_window_heatmap(exact_time_dir, time_analysis, window_size_seconds)\n",
    "        \n",
    "        # HEATMAP 2: Temporal Distribution of High Correlations\n",
    "        self._create_temporal_distribution_heatmap(exact_time_dir, time_analysis, window_size_seconds)\n",
    "        \n",
    "        # HEATMAP 3: Region Pair Activity Timeline\n",
    "        self._create_region_timeline_heatmap(exact_time_dir, time_analysis, window_size_seconds)\n",
    "        \n",
    "        # Generate detailed time window report\n",
    "        self._generate_time_window_report(exact_time_dir, time_analysis)\n",
    "        \n",
    "        print(f\"All exact time heatmaps saved in: {exact_time_dir}\")\n",
    "        return exact_time_dir\n",
    "\n",
    "    def _create_time_window_heatmap(self, output_dir, time_analysis, window_size_seconds):\n",
    "        \"\"\"\n",
    "        Create heatmap showing which region pairs have high correlations in which time windows\n",
    "        \"\"\"\n",
    "        print(\"Creating time window vs region pairs heatmap...\")\n",
    "        \n",
    "        # Create matrix: time_windows × region_pairs\n",
    "        num_windows = len(time_analysis['windows'])\n",
    "        num_pairs = self.num_regions * (self.num_regions - 1) // 2  # Unique pairs\n",
    "        \n",
    "        # Initialize heatmap matrix\n",
    "        heatmap_matrix = np.zeros((num_windows, num_pairs))\n",
    "        pair_labels = []\n",
    "        \n",
    "        # Create mapping from (i,j) to pair index\n",
    "        pair_index_map = {}\n",
    "        pair_idx = 0\n",
    "        for i in range(self.num_regions):\n",
    "            for j in range(i+1, self.num_regions):\n",
    "                pair_index_map[(i, j)] = pair_idx\n",
    "                pair_labels.append(f\"R{i}-R{j}\")\n",
    "                pair_idx += 1\n",
    "        \n",
    "        # Fill heatmap matrix\n",
    "        for window_idx, window_data in enumerate(time_analysis['windows']):\n",
    "            for pair_data in window_data['high_correlation_pairs']:\n",
    "                i = pair_data['region_i']\n",
    "                j = pair_data['region_j']\n",
    "                pair_key = (min(i, j), max(i, j))  # Ensure consistent ordering\n",
    "                if pair_key in pair_index_map:\n",
    "                    heatmap_matrix[window_idx, pair_index_map[pair_key]] = pair_data['correlation']\n",
    "        \n",
    "        # Create the heatmap\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Use only pairs that have at least one high correlation\n",
    "        active_pairs_mask = np.any(heatmap_matrix != 0, axis=0)\n",
    "        active_heatmap = heatmap_matrix[:, active_pairs_mask]\n",
    "        active_labels = [pair_labels[i] for i in range(len(pair_labels)) if active_pairs_mask[i]]\n",
    "        \n",
    "        if active_heatmap.size == 0:\n",
    "            print(\"No high correlation pairs found for heatmap\")\n",
    "            return\n",
    "        \n",
    "        im = plt.imshow(active_heatmap.T, aspect='auto', cmap='RdYlBu_r', \n",
    "                       vmin=-1, vmax=1, interpolation='nearest')\n",
    "        \n",
    "        # Set labels\n",
    "        time_labels = [f\"{w['start_time']}-{w['end_time']}s\" for w in time_analysis['windows']]\n",
    "        plt.xticks(np.arange(len(time_labels)), time_labels, rotation=45, ha='right')\n",
    "        plt.yticks(np.arange(len(active_labels)), active_labels)\n",
    "        \n",
    "        plt.xlabel(f'Time Windows ({window_size_seconds} seconds each)', fontweight='bold', fontsize=12)\n",
    "        plt.ylabel('Region Pairs', fontweight='bold', fontsize=12)\n",
    "        plt.title(f'EXACT TIME CORRELATION HEATMAP\\n{self.video_name}\\n'\n",
    "                 f'High Correlations (> {time_analysis[\"correlation_threshold\"]}) in {window_size_seconds}-Second Windows',\n",
    "                 fontweight='bold', fontsize=14, pad=20)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, label='Correlation Coefficient')\n",
    "        cbar.set_label('Correlation Coefficient', fontweight='bold')\n",
    "        \n",
    "        # Add grid\n",
    "        plt.grid(True, alpha=0.3, color='black')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        heatmap_path = os.path.join(output_dir, f'{self.video_name}_exact_time_correlation_heatmap.png')\n",
    "        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✓ Exact time correlation heatmap saved: {heatmap_path}\")\n",
    "\n",
    "    def _create_temporal_distribution_heatmap(self, output_dir, time_analysis, window_size_seconds):\n",
    "        \"\"\"\n",
    "        Create heatmap showing temporal distribution of high correlations\n",
    "        \"\"\"\n",
    "        print(\"Creating temporal distribution heatmap...\")\n",
    "        \n",
    "        # Count high correlations per time window\n",
    "        high_corr_counts = [len(window['high_correlation_pairs']) for window in time_analysis['windows']]\n",
    "        time_windows = [f\"{w['start_time']}-{w['end_time']}s\" for w in time_analysis['windows']]\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(fsize=(16, 8))\n",
    "        \n",
    "        bars = plt.bar(range(len(high_corr_counts)), high_corr_counts, \n",
    "                      color='red', alpha=0.7, edgecolor='darkred')\n",
    "        \n",
    "        plt.xlabel(f'Time Windows ({window_size_seconds} seconds each)', fontweight='bold', fontsize=12)\n",
    "        plt.ylabel('Number of High Correlation Pairs', fontweight='bold', fontsize=12)\n",
    "        plt.title(f'TEMPORAL DISTRIBUTION OF HIGH CORRELATIONS\\n{self.video_name}\\n'\n",
    "                 f'Number of Region Pairs with Correlation > {time_analysis[\"correlation_threshold\"]} in Each Time Window',\n",
    "                 fontweight='bold', fontsize=14, pad=20)\n",
    "        \n",
    "        plt.xticks(range(len(time_windows)), time_windows, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, high_corr_counts):\n",
    "            if count > 0:\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        dist_path = os.path.join(output_dir, f'{self.video_name}_temporal_distribution.png')\n",
    "        plt.savefig(dist_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✓ Temporal distribution heatmap saved: {dist_path}\")\n",
    "\n",
    "    def _create_region_timeline_heatmap(self, output_dir, time_analysis, window_size_seconds):\n",
    "        \"\"\"\n",
    "        Create heatmap showing when each region is active in high correlations\n",
    "        \"\"\"\n",
    "        print(\"Creating region activity timeline heatmap...\")\n",
    "        \n",
    "        # Create matrix: regions × time_windows\n",
    "        activity_matrix = np.zeros((self.num_regions, len(time_analysis['windows'])))\n",
    "        \n",
    "        for window_idx, window_data in enumerate(time_analysis['windows']):\n",
    "            for pair_data in window_data['high_correlation_pairs']:\n",
    "                i = pair_data['region_i']\n",
    "                j = pair_data['region_j']\n",
    "                activity_matrix[i, window_idx] += 1\n",
    "                activity_matrix[j, window_idx] += 1\n",
    "        \n",
    "        # Create the heatmap\n",
    "        plt.figure(figsize=(18, 10))\n",
    "        \n",
    "        im = plt.imshow(activity_matrix, aspect='auto', cmap='YlOrRd', \n",
    "                       interpolation='nearest', vmin=0, vmax=np.max(activity_matrix))\n",
    "        \n",
    "        # Set labels\n",
    "        time_labels = [f\"{w['start_time']}-{w['end_time']}s\" for w in time_analysis['windows']]\n",
    "        region_labels = [f\"R{i}\" for i in range(self.num_regions)]\n",
    "        \n",
    "        plt.xticks(np.arange(len(time_labels)), time_labels, rotation=45, ha='right')\n",
    "        plt.yticks(np.arange(len(region_labels)), region_labels)\n",
    "        \n",
    "        plt.xlabel(f'Time Windows ({window_size_seconds} seconds each)', fontweight='bold', fontsize=12)\n",
    "        plt.ylabel('Regions', fontweight='bold', fontsize=12)\n",
    "        plt.title(f'REGION ACTIVITY TIMELINE\\n{self.video_name}\\n'\n",
    "                 f'Number of High Correlation Involvements per Region in Each Time Window',\n",
    "                 fontweight='bold', fontsize=14, pad=20)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, label='Number of High Correlation Involvements')\n",
    "        cbar.set_label('Number of High Correlation Involvements', fontweight='bold')\n",
    "        \n",
    "        # Add values in cells\n",
    "        for i in range(activity_matrix.shape[0]):\n",
    "            for j in range(activity_matrix.shape[1]):\n",
    "                if activity_matrix[i, j] > 0:\n",
    "                    plt.text(j, i, f'{int(activity_matrix[i, j])}', \n",
    "                            ha=\"center\", va=\"center\", color=\"white\" if activity_matrix[i, j] > np.max(activity_matrix)/2 else \"black\",\n",
    "                            fontweight='bold')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3, color='black')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        timeline_path = os.path.join(output_dir, f'{self.video_name}_region_activity_timeline.png')\n",
    "        plt.savefig(timeline_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✓ Region activity timeline saved: {timeline_path}\")\n",
    "\n",
    "    def _generate_time_window_report(self, output_dir, time_analysis):\n",
    "        \"\"\"\n",
    "        Generate detailed report showing exact time windows with high correlations\n",
    "        \"\"\"\n",
    "        print(\"Generating detailed time window report...\")\n",
    "        \n",
    "        # Create summary statistics\n",
    "        total_events = len(time_analysis['high_correlation_events'])\n",
    "        active_windows = sum(1 for window in time_analysis['windows'] if window['high_correlation_pairs'])\n",
    "        \n",
    "        # Find most active time windows\n",
    "        window_activity = []\n",
    "        for window in time_analysis['windows']:\n",
    "            if window['high_correlation_pairs']:\n",
    "                max_corr = max([abs(pair['correlation']) for pair in window['high_correlation_pairs']])\n",
    "                window_activity.append({\n",
    "                    'time_window': f\"{window['start_time']}-{window['end_time']}s\",\n",
    "                    'start_time': window['start_time'],\n",
    "                    'pair_count': len(window['high_correlation_pairs']),\n",
    "                    'max_correlation': max_corr\n",
    "                })\n",
    "        \n",
    "        # Sort by activity\n",
    "        window_activity.sort(key=lambda x: x['pair_count'], reverse=True)\n",
    "        \n",
    "        # Find strongest correlations\n",
    "        strong_events = sorted(time_analysis['high_correlation_events'], \n",
    "                              key=lambda x: x['absolute_correlation'], reverse=True)[:20]\n",
    "        \n",
    "        # Compile report\n",
    "        time_report = {\n",
    "            'summary': {\n",
    "                'total_high_correlation_events': total_events,\n",
    "                'total_time_windows': len(time_analysis['windows']),\n",
    "                'active_time_windows': active_windows,\n",
    "                'correlation_threshold': time_analysis['correlation_threshold'],\n",
    "                'window_size_seconds': time_analysis['window_size_seconds'],\n",
    "                'percentage_active_windows': (active_windows / len(time_analysis['windows'])) * 100\n",
    "            },\n",
    "            'most_active_windows': window_activity[:10],\n",
    "            'strongest_correlations': [\n",
    "                {\n",
    "                    'time_window': event['time_window'],\n",
    "                    'region_pair': f\"R{event['region_i']}-R{event['region_j']}\",\n",
    "                    'correlation': event['correlation'],\n",
    "                    'absolute_correlation': event['absolute_correlation'],\n",
    "                    'direction': event['direction']\n",
    "                }\n",
    "                for event in strong_events\n",
    "            ],\n",
    "            'detailed_windows': []\n",
    "        }\n",
    "        \n",
    "        # Add detailed window information (only active windows)\n",
    "        for window in time_analysis['windows']:\n",
    "            if window['high_correlation_pairs']:\n",
    "                window_detail = {\n",
    "                    'time_window': f\"{window['start_time']}-{window['end_time']}s\",\n",
    "                    'start_time_seconds': window['start_time'],\n",
    "                    'end_time_seconds': window['end_time'],\n",
    "                    'high_correlation_pairs': [\n",
    "                        {\n",
    "                            'region_pair': f\"R{pair['region_i']}-R{pair['region_j']}\",\n",
    "                            'correlation': pair['correlation'],\n",
    "                            'direction': pair['direction']\n",
    "                        }\n",
    "                        for pair in window['high_correlation_pairs']\n",
    "                    ]\n",
    "                }\n",
    "                time_report['detailed_windows'].append(window_detail)\n",
    "        \n",
    "        # Save JSON report\n",
    "        report_path = os.path.join(output_dir, f'{self.video_name}_exact_time_report.json')\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(time_report, f, indent=2)\n",
    "        \n",
    "        # Save CSV report for easy analysis\n",
    "        csv_data = []\n",
    "        for event in time_analysis['high_correlation_events']:\n",
    "            csv_data.append({\n",
    "                'Time_Window': event['time_window'],\n",
    "                'Start_Time_Seconds': event['start_time_seconds'],\n",
    "                'End_Time_Seconds': event['end_time_seconds'],\n",
    "                'Region_I': event['region_i'],\n",
    "                'Region_J': event['region_j'],\n",
    "                'Region_Pair': f\"R{event['region_i']}-R{event['region_j']}\",\n",
    "                'Correlation': event['correlation'],\n",
    "                'Absolute_Correlation': event['absolute_correlation'],\n",
    "                'Direction': event['direction']\n",
    "            })\n",
    "        \n",
    "        if csv_data:\n",
    "            df = pd.DataFrame(csv_data)\n",
    "            csv_path = os.path.join(output_dir, f'{self.video_name}_exact_time_correlations.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_time_window_summary(time_report)\n",
    "        \n",
    "        print(f\"✓ Detailed time window report saved: {report_path}\")\n",
    "        if csv_data:\n",
    "            print(f\"✓ CSV data saved: {csv_path}\")\n",
    "        \n",
    "        return time_report\n",
    "\n",
    "    def _print_time_window_summary(self, time_report):\n",
    "        \"\"\"Print summary of time window analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXACT TIME WINDOW ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        summary = time_report['summary']\n",
    "        print(f\"Total High Correlation Events: {summary['total_high_correlation_events']}\")\n",
    "        print(f\"Active Time Windows: {summary['active_time_windows']}/{summary['total_time_windows']} ({summary['percentage_active_windows']:.1f}%)\")\n",
    "        print(f\"Correlation Threshold: > {summary['correlation_threshold']}\")\n",
    "        print(f\"Window Size: {summary['window_size_seconds']} seconds\")\n",
    "        \n",
    "        if time_report['most_active_windows']:\n",
    "            print(f\"\\nMost Active Time Windows:\")\n",
    "            for i, window in enumerate(time_report['most_active_windows'][:5], 1):\n",
    "                print(f\"  {i}. {window['time_window']}: {window['pair_count']} high correlation pairs (max: {window['max_correlation']:.3f})\")\n",
    "        \n",
    "        if time_report['strongest_correlations']:\n",
    "            print(f\"\\nStrongest Correlations:\")\n",
    "            for i, corr in enumerate(time_report['strongest_correlations'][:5], 1):\n",
    "                print(f\"  {i}. {corr['time_window']} - {corr['region_pair']}: {corr['correlation']:.3f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "    def generate_comprehensive_temporal_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive report including exact time window analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"GENERATING COMPREHENSIVE TEMPORAL ANALYSIS REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Ensure all analyses are performed\n",
    "        self.extract_video_information()\n",
    "        self.compute_movement_intensity()\n",
    "        \n",
    "        # Perform temporal delay analysis\n",
    "        self.compute_temporal_delay_correlation(max_time_lag=10)\n",
    "        \n",
    "        # Create exact time window analysis\n",
    "        exact_time_dir = self.create_exact_time_heatmaps(correlation_threshold=0.5, window_size_seconds=10)\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        if self.temporal_delay_analysis:\n",
    "            significant_pairs = np.sum(self.temporal_delay_analysis['significant_correlations'])\n",
    "            high_significant_pairs = np.sum(self.temporal_delay_analysis['high_significant_correlations'])\n",
    "            \n",
    "            # Get time window analysis results\n",
    "            time_report = self.time_window_analysis\n",
    "            \n",
    "            # Compile comprehensive report\n",
    "            comprehensive_report = {\n",
    "                'video_metadata': self.video_info,\n",
    "                'analysis_parameters': {\n",
    "                    'spatial_grid': f\"{self.spatial_grid[0]}x{self.spatial_grid[1]}\",\n",
    "                    'temporal_segment_seconds': self.temporal_segment,\n",
    "                    'total_regions': self.num_regions,\n",
    "                    'total_segments': self.movement_matrix.shape[1],\n",
    "                    'max_time_lag_seconds': 10,\n",
    "                    'significance_threshold': 0.3,\n",
    "                    'high_significance_threshold': 0.5,\n",
    "                    'time_window_size_seconds': 10\n",
    "                },\n",
    "                'global_correlation_results': {\n",
    "                    'significant_correlation_pairs': int(significant_pairs),\n",
    "                    'high_significant_correlation_pairs': int(high_significant_pairs),\n",
    "                    'total_possible_pairs': self.num_regions * (self.num_regions - 1),\n",
    "                    'average_correlation_strength': float(np.mean(np.abs(self.temporal_delay_analysis['max_correlation_matrix']))),\n",
    "                    'average_absolute_time_lag_seconds': float(np.mean(np.abs(self.temporal_delay_analysis['optimal_time_lag_matrix']))),\n",
    "                    'percentage_significant_pairs': float(significant_pairs / (self.num_regions * (self.num_regions - 1)) * 100),\n",
    "                    'percentage_high_significant_pairs': float(high_significant_pairs / (self.num_regions * (self.num_regions - 1)) * 100)\n",
    "                },\n",
    "                'time_window_analysis': time_report\n",
    "            }\n",
    "            \n",
    "            # Save comprehensive report\n",
    "            report_path = os.path.join(self.output_dir, f'{self.video_name}_comprehensive_temporal_report.json')\n",
    "            with open(report_path, 'w') as f:\n",
    "                json.dump(comprehensive_report, f, indent=2)\n",
    "            \n",
    "            print(f\"Comprehensive temporal report saved: {report_path}\")\n",
    "            \n",
    "            # Print comprehensive summary\n",
    "            self._print_comprehensive_summary(comprehensive_report)\n",
    "            \n",
    "            return comprehensive_report\n",
    "\n",
    "    def _print_comprehensive_summary(self, report):\n",
    "        \"\"\"Print comprehensive summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE TEMPORAL ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        global_results = report['global_correlation_results']\n",
    "        time_window = report['time_window_analysis']['summary']\n",
    "        \n",
    "        print(f\"Video Duration: {report['video_metadata']['duration']:.1f}s\")\n",
    "        print(f\"Spatial Grid: {report['analysis_parameters']['spatial_grid']}\")\n",
    "        print(f\"Time Window Size: {report['analysis_parameters']['time_window_size_seconds']}s\")\n",
    "        \n",
    "        print(f\"\\nGLOBAL CORRELATION RESULTS:\")\n",
    "        print(f\"  High Correlation Pairs (>0.5): {global_results['high_significant_correlation_pairs']} ({global_results['percentage_high_significant_pairs']:.1f}%)\")\n",
    "        print(f\"  Average Correlation: {global_results['average_correlation_strength']:.3f}\")\n",
    "        print(f\"  Average Time Lag: {global_results['average_absolute_time_lag_seconds']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\nEXACT TIME WINDOW ANALYSIS:\")\n",
    "        print(f\"  Total High Correlation Events: {time_window['total_high_correlation_events']}\")\n",
    "        print(f\"  Active Windows: {time_window['active_time_windows']}/{time_window['total_time_windows']} ({time_window['percentage_active_windows']:.1f}%)\")\n",
    "        \n",
    "        # Show top time windows with high correlations\n",
    "        if report['time_window_analysis']['most_active_windows']:\n",
    "            print(f\"\\nTOP ACTIVE TIME WINDOWS:\")\n",
    "            for i, window in enumerate(report['time_window_analysis']['most_active_windows'][:3], 1):\n",
    "                print(f\"  {i}. {window['time_window']}: {window['pair_count']} high correlation pairs\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "# Main execution function with proper video discovery\n",
    "def run_comprehensive_temporal_analysis(video_path, output_dir=None, spatial_grid=(4, 4), temporal_segment=1.0):\n",
    "    \"\"\"\n",
    "    Run complete comprehensive temporal analysis including exact time windows\n",
    "    \"\"\"\n",
    "    analyzer = SpatialTemporalCrossCorrelation(video_path, output_dir, spatial_grid, temporal_segment)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starting COMPREHENSIVE temporal analysis for: {analyzer.video_name}\")\n",
    "        print(f\"Including EXACT TIME WINDOW analysis (0-10s, 10-20s, etc.)\")\n",
    "        \n",
    "        # Run comprehensive analysis\n",
    "        report = analyzer.generate_comprehensive_temporal_report()\n",
    "        \n",
    "        return analyzer, report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comprehensive temporal analysis failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to find and process videos in your folder\n",
    "def find_and_process_videos(video_folder, output_base_dir=\"comprehensive_temporal_results\"):\n",
    "    \"\"\"\n",
    "    Find all videos in the specified folder and process them\n",
    "    \"\"\"\n",
    "    print(f\"Looking for videos in: {video_folder}\")\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(video_folder):\n",
    "        print(f\"❌ Error: Video folder '{video_folder}' does not exist!\")\n",
    "        return []\n",
    "    \n",
    "    # Supported video formats\n",
    "    video_extensions = ('.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.MP4', '.AVI', '.MKV', '.MOV')\n",
    "    \n",
    "    # Find all video files\n",
    "    video_files = []\n",
    "    for file in os.listdir(video_folder):\n",
    "        if file.lower().endswith(video_extensions):\n",
    "            video_files.append(file)\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"❌ No video files found in {video_folder}\")\n",
    "        print(f\"Supported formats: {video_extensions}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"✅ Found {len(video_files)} video files:\")\n",
    "    for video_file in video_files:\n",
    "        print(f\"  - {video_file}\")\n",
    "    \n",
    "    # Process each video\n",
    "    results = []\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        output_dir = os.path.join(output_base_dir, video_name)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"PROCESSING: {video_file}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        try:\n",
    "            analyzer, report = run_comprehensive_temporal_analysis(\n",
    "                video_path=video_path,\n",
    "                output_dir=output_dir,\n",
    "                spatial_grid=(4, 4),\n",
    "                temporal_segment=1.0\n",
    "            )\n",
    "            results.append((video_file, analyzer, report))\n",
    "            print(f\"✅ Successfully processed: {video_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process {video_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage with your actual folder path\n",
    "if __name__ == \"__main__\":\n",
    "    # Use your actual video folder path\n",
    "    video_folder = \"/mnt/d/TileClipper_Implementation/data/video\"\n",
    "    output_base_dir = \"comprehensive_temporal_results\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SPATIAL-TEMPORAL CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Video folder: {video_folder}\")\n",
    "    print(f\"Output directory: {output_base_dir}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find and process all videos\n",
    "    results = find_and_process_videos(video_folder, output_base_dir)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n🎉 ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"✅ Processed {len(results)} videos\")\n",
    "        print(f\"📁 Results saved in: {output_base_dir}\")\n",
    "        \n",
    "        # Print summary for each video\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for video_file, analyzer, report in results:\n",
    "            print(f\"\\n📊 {video_file}:\")\n",
    "            if hasattr(analyzer, 'time_window_analysis'):\n",
    "                time_report = analyzer.time_window_analysis['summary']\n",
    "                print(f\"  - Duration: {analyzer.video_info['duration']:.1f}s\")\n",
    "                print(f\"  - High correlation events: {time_report['total_high_correlation_events']}\")\n",
    "                print(f\"  - Active time windows: {time_report['active_time_windows']}/{time_report['total_time_windows']}\")\n",
    "                \n",
    "                # Show specific time windows with high correlations\n",
    "                active_windows = [w for w in analyzer.time_window_analysis['windows'] if w['high_correlation_pairs']]\n",
    "                if active_windows:\n",
    "                    print(f\"  - Top time windows with high correlations:\")\n",
    "                    for window in active_windows[:3]:  # Show first 3 active windows\n",
    "                        print(f\"    • {window['start_time']}-{window['end_time']}s: {len(window['high_correlation_pairs'])} pairs\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No videos were successfully processed.\")\n",
    "        print(f\"Please check:\")\n",
    "        print(f\"  1. The folder path: {video_folder}\")\n",
    "        print(f\"  2. Video file formats (supported: mp4, avi, mkv, mov, wmv, flv, webm)\")\n",
    "        print(f\"  3. File permissions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
